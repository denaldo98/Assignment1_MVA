---
title: "MCA applied to Bicing data"
author: "Denaldo Lapi, Samy Chouti, Franscesco Aristei"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_notebook: default
  html_document:
    df_print: paged
  pdf_document: default
---

```{r}
# delete R objects left in memory
rm(list = ls())
```

```{r}
# install needed packages
#install.packages(c("FactoMineR", "factoextra", "dplyr"))
```

```{r, results='hide'}
# load libraries
library(FactoMineR)
library(factoextra)
library(dplyr)
```

## Exploratory data analysis

```{r}
# load the data
bicing <- read.csv(file='bicing.csv', header=TRUE, sep=";", dec=",")
```

Before applying MCA, let's first explore the dataset.

```{r}
# inspect the dataset
head(bicing)
```

By reading the columns, we can understand that this dataset has been made from data gathered by the Bicing company, offering bike rental service in Barcelona (or a similary bike rental company).

Each of the 4.877 rows of the dataset represents a rental with information such as:

-   Rental start and end date (Start.date, Start.time, End.date, End.time, Total.duration, ...)
-   Information on the start and end stations (identification with Start.station.number or End.station.number, position and altitude with Altitude.E or End.lat for example)
-   Weather information, although we don't know if the data is related to the start or end station (or not any of those). We will assume that it is the weather in the city over the day.

Let's see a brief summary of the main statistics of the dataset:

```{r}
summary(bicing)
```

About our variables' type, we have categorical variables: \* Start.weekday, Weekend, Nightout, Period \* Metro.station.S, Metro.station.E, Metro.sORe and Metro.sANDe \* Loop \* Account.type \* Weather.main \* Downtown.S and Downtown.E

## MCA

We'll perform multiple correspondence analysis (MCA) by considering a ventilation level of 1% for the infrequent categories and saving the results of the first 10 dimensions. We'll use as active variables: "Start.weekday", "Period", "Metro.station.E", "Downtown.E", "Account.type" and "Weather.main". We define "Month" and "Temperature" as supplementary variables.

Since we are not using all the variables as active, we will create a new dataframe with only the relevant columns for our analysis:

```{r}
X2 = bicing[c("Start.weekday", "Period", "Metro.station.E", "Downtown.E",  "Account.type", "Weather.main", "Month", "Temperature")] 
```

```{r}
X2
```

The new dataframe has 4877 individuals ( I ) and 8 columns: the first 6 will be our active categorical variables for the MCA ( J ), while "Month" will be threated as a supplementary categorical variable, and "Temperature" as a supplementary quantitative variable.

Let's now set the correct types for the column variables

```{r}
summary(X2["Month"])
```

As we can see from above, the variable "Month" is threated as a quantitative variable, while it assumes only 8 discrete values as shown below:

```{r}
unique(X2[, "Month"])
```

To solve this issue we transform it into a categorical variable:

```{r}
X2[,"Month"] = as.factor(X2[,"Month"])
```

We now transform all the active variables into categorical ones:

```{r}
X2[,"Period"] = as.factor(X2[,"Period"])
X2[,"Start.weekday"] = as.factor(X2[,"Start.weekday"])
X2[,"Metro.station.E"] = as.factor(X2[,"Metro.station.E"])
X2[,"Downtown.E"] = as.factor(X2[,"Downtown.E"])
X2[,"Account.type"] = as.factor(X2[,"Account.type"])
X2[,"Weather.main"] = as.factor(X2[,"Weather.main"])
```

```{r}
print(unique(X2[, "Period"]))
print(unique(X2[, "Start.weekday"]))
print(unique(X2[, "Metro.station.E"]))
print(unique(X2[, "Downtown.E"]))
print(unique(X2[, "Account.type"]))
print(unique(X2[, "Weather.main"]))
```

Let's see now the summary of our dataframe

```{r}
summary(X2)
```

It's also possible to plot the frequency of variable categories

```{r}
for (i in 1:6) {
  plot(X2[,i], main=colnames(X2)[i],
       ylab = "Count", col="steelblue", las = 2)
  }
```

We are now ready to perform MCA

```{r}
?MCA
```

We use the MCA function of the FactoMineR package and we specify the number of dimensions to keep in the final results by means of the parameter ncp.

```{r}
bicing_mca = MCA(X2, ncp=10, quali.sup = 7,  quanti.sup=8, level.ventil=0.01, graph = FALSE)
```

## Questions

***What is the number of dimensions that this MCA generates? What does this value equal?***

```{r}
# Let's visualize the eigenvalues
bicing_mca$eig
```

MCA generates 16 dimensions which all together explain 100% of the variance of the original data.

We know that the number of dimensions generated by MCA is given by $K - J$, where $K$ represents the total number of categories of the active categorical variables, while $J$ represents the number of active categorical variables.

In our case $J$ is equal to 6, while $K$ is equal to the total sum of the categories of each active categorical variable:

```{r}
unique_count = c()
for(col in 1:(length(colnames(X2)) - 2)){
  unique_count = c(unique_count, length(unique(X2[,colnames(X2)[col]])))
}
sum(unique_count)
```

Therefore, we should obtain $K-J=26-6=20$ dimensions/eigenvalues. However, as we are applying ventilation (of 1%), 4 categories as removed for being rare which leaves us with 16 categories.

For the rest of the exploitation, we will only keep the 10 first dimensions as we specified when computing MCA (through the `ncp` argument).

### Visualization and Interpretation

Let's now analyze the insight of the MCA computation.

### Plots

**Interpret the first factorial plane (dimensions 1 and 2) by means of variables and categories**

Let's first interpret the variable categories plot on the first 2 dimensions (we use the 'fviz_mca_var' function of the factoextra package):

```{r}
fviz_mca_var(bicing_mca, repel = TRUE, ggtheme = theme_minimal())
```

On this plot, we can see how much variable categories are correlated to each others. For example, we can see a strong negative correlation between `Saturday` and `Morning` as being opposed compared to the origin of the plot. This means that a very few profiles were combining those two variables, if not none.\
**Contextualization:** A very few bike rentals were made on Satursday mornings, compared to other rentals.

Also, we can see that there is a relatively strong correlation between between `Friday` and `Wednesday` variables or between `Showers` and `Monday`.\
**Contextualization:** Profiles of bike rentals of Friday and Wednesday are pretty similar, which could be due to populations going out at night and renting bikes to do so (as it is less frequent to go out at the beginning of the week).

Let's visualize the correlation between variables and MCA principal dimensions:

```{r}
# plot the correlations between variables and 1st factorial plane
fviz_mca_var(bicing_mca, choice="var", repel = TRUE, ggtheme = theme_minimal())
```

On this plot, we can see how each variables are correlating with the 2 first dimensions. For example, we can see that `Weather.main` is strongly correlated with the second dimension when the supplementary variable `Temperature` is having a low correlation with both dimensions, thus not having a strong relation on this dimension. Also, there are variables such as `Period` or `Start.weekday` which have a mixed correlation with both dimensions.

#### Choosing dimensions

Let's display the bar plot of the percentage of variance explained by each component

We can see that the computed dimensions have all of them a relatively low portion of variation explanation (In order to not be biased during reading this plot, please pay attention to the y axis range):

```{r}
fviz_screeplot(bicing_mca, addlabels = TRUE, ylim = c (0, 10))
```

**Which percentage of variability is explained by the first two dimensions?** For example, for the above interpretation of variables and categories, we used the two first dimensions. Even if they are the most explaining in terms of variance, they only account for 16% of variance in total.

We can print the eigenvalues and the variance explained by each dimension:

```{r}
bicing_mca$eig
```

Therefore, these dimensions (the first 2) are not so representative of the whole variance.

**Decide the number of significant dimensions that you retain.** The question is: how can we keep significant dimensions without loosing on representation quality (as too much dimension makes representation harder) ?

In order to decide the number of significant dimensions to retain, we know that we can use an empirical formula, which states that a good criteria is to interpret the axes of inertia above 1/**J** , where **J** is equal to the number of active categorical variables, which is 6 in our case

```{r}
sum(bicing_mca$eig[,1]>(1/6))
```

Therefore, according to the empirical formula we should keep the first 8 dimensions.

**Discuss which modalities contribute most to the first and second dimensions**
Let's show the bar plot of the contribution to the first dimension (Morning is first): \> Please note that we will negate the input for each `rank` function uses in order to get descendant rankings and not the ascendant ranking that is offering the fuction by default.

```{r}
fviz_contrib (bicing_mca, choice="var", axes=1, top=10)
```

And to the second dimension (Clear is first):

```{r}
fviz_contrib (bicing_mca, choice="var", axes=2, top=10)
```

We can see that:\
\* For the first dimension: we first note that variable categories have sparse rankings. It is mostly `Period` (with `Morning` 1st and `Night` 5th), `Account.type` (with Casual as 2nd) and `Start.weekday` (with `Saturday` 3rd and `Sunday` 4th) that are the top contributors to this dimension. \* For the second dimension: the variable categories are less spreaded than in the first dimension. Top contributors are Weather (with `Clear` 1st and `Cloudy` 2nd) then `Period` (with `Night` 3rd and `Afternoon` 4th) and `Start.weekday` (Friday 5th and Monday 6th).

We can also compare both dimension correlation. In overall, Period is the top contributor to both dimension (Morning/Night for the 1st, Night/Afternoon for the 2nd), followed by Start.weekday although contributing with different days (Saturday/Sunday for the 1st, Friday/Monday for the 2nd).

It is normal that there is not that much correlation beween those two dimensions as the goal of computing those dimensions is to maximize variance explaination thus reducing correlation at maximum (or having those two dimensions would be useless as giving the same insights). Here is the correlation score (-0.17) which we expected to be low:

```{r}
cor(bicing_mca$var$contrib[,1:2])
```

### What about variables representation ?

Above we interpreted the contribution of each variables in the dimensions using the eigenvalue decomposition and others. But how can we ensure that each variable are well represented in those dimensions ?\
This is where the `cos2` score becomes relevant: it measures each variables' representation, as can be seen on the below plot (for dim1 and 2):

```{r}
fviz_mca_var(bicing_mca, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE, 
             ggtheme = theme_minimal())
```

The more the cos2 score is higher (red), the more the variable is being represented in the dimension.

**For the other variables, identify the dimensions where the categories of "Metro.station.E" and "Downtown.E" are best represented.** To do so, as before, we will use the `rank` function to find dimensions representing the most these two variables `Metro.station.E` and `Downtown.E`. As can be seen below, we can just keep the cos2 score of `Metro.station.E_TRUE` and `Downtown.E_TRUE` as it is the same for the whole variable:

```{r}
bicing_mca$var$cos2[13:16,]
```

Therefore, here are the rankings for `Metro.station.E`:

```{r}
rank(-bicing_mca$var$cos2[13,])
bicing_mca$var$cos2[13,]
```

*Dim7 and Dim10 are the most representative* of `Metro.station.E` with respectively 0.29 and 0.27 cos2 score when Dim4 (3rd position) only have 0.11, which makes it less representative than the two first dimensions.

And the same for `Downtown.E`:

```{r}
rank(-bicing_mca$var$cos2[15,])
bicing_mca$var$cos2[15,]
```

*Dim3 is clearly the most representative* of `Downtown.E` with a 0.33. The second most representative dimension is the Dim1 with a 0.13 score which is way lower (and its even worse with the third most representative dimension Dim2 with 0.04 cos2 score).

### Supplementary variables

**Are the supplementary variables related to the principal axes? In which way?**

#### Supplementary quantitative variables 

Let's first analyze the `Temperature` correlations with the first 2 dimensions.

```{r}
fviz_mca_var(bicing_mca, choice = "quanti.sup", ggtheme = theme_minimal())
```

We can see that the two principal dimensions are not so influenced by the `Temperature` variables. We could also check if they are more for other dimensions:

```{r}
bicing_mca$quanti.sup
```

which is not the case as the highest contribution is from Dim6 and is very similar to Dim2's contribution (around 0.168). Therefore we can conclude that the supplementary quantitative variable is not well contributing to the 10 first MCA dimensions.

#### Supplementary qualitative variables

What about the `Month` variable ?

Let's see the coordinates on the 1st factorial plane:

```{r}
plot.MCA(bicing_mca, invisible=c("ind", "var", "ind.sup", "quanti.sup"))
```

We can see the correlation of this supplementary variable with the 1st and the 2nd dimension:

```{r}
# plot the correlations between variables and 1st factorial plane
fviz_mca_var(bicing_mca, choice="var", repel = TRUE, ggtheme = theme_minimal())
```

We can see that month is not strongly correlated with the first 2 dimensions.

```{r}
bicing_mca$quali.sup
```

And the maximum influence:

```{r}
max(bicing_mca$quali.sup$coord)
```

With that maximum influence score in Dim7 for November, we can see that it have a relatively strong influence but not on principal axis, therefore being out of scope for this question.

**General conclusion about supplementary variables**\
Whether it is quantitative or qualitative variables, we can see that they are not really related to principal dimensions Dim1 and Dim2 (but can be with other dimensions, as explained above).
