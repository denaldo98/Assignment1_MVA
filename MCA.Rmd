---
title: "MCA applied to Bicing data"
output:
  html_notebook: default
  html_document:
    df_print: paged
  pdf_document: default
---

```{r}
# install needed packages
#install.packages(c("FactoMineR", "factoextra", "dplyr"))
```

```{r}
# load libraries
library(FactoMineR)
library(factoextra)
library(dplyr)
```

## Exploratory data analysis

```{r}
bicing <- read.csv(file='bicing.csv', header=TRUE, sep=";", dec=",")
```

Before applying MCA, let's first explore the dataset.

```{r}
# inspect the dataset
bicing
```

By reading the columns, we can understand that this dataset has been made from data gathered by the Bicing company, offering bike rental service in Barcelona.

Each row of the dataset represents a rental with information such as:

-   Rental start and end date (Start.date, Start.time, End.date, End.time, Total.duration, ...)

-   Information on the start and end stations (identification with Start.station.number or End.station.number, position and altitude with Altitude.E or End.lat for example)

-   Weather information, although we don't know if the data is related to the start or end station (or not any of those). We will assume that it is the weather in the city over the day.

    We have in total 4.877 individuals

Let's see a brief summary of the main statistics of the dataset:

```{r}
summary(bicing)
```

About our variables' type, we have categorical variables: \* Start.weekday, Weekend, Nightout, Period \* Metro.staion.S, Metro.station.E, Metro.sORe and Metro.sANDe \* Loop \* Account.type \* Weather.main \* Downtown.S and Downtown.E

## MCA

We'll perform multiple correspondence analysis (MCA) by considering a ventilation level of 1% for the
infrequent categories and saving the results of the first 10 dimensions. We'll use as active variables:
"Start.weekday", "Period", "Metro.station.E", "Downtown.E", "Account.type" and
"Weather.main". Define "Month" and "Temperature" as supplementary variables.

Since we are not using all the variables as active, we will create a new dataframe with only the relevant columns for our analysis:

```{r}
#X = bicing[c("Start.weekday", "Weekend", "Nightout", "Period",   "Metro.station.S", "Metro.station.E", "Metro.sORe", "Metro.sANDe"        , "Downtown.S", "Downtown.E", "Loop", "Account.type", "Weather.main")]

#bicing_mca = MCA(X, ncp=10, quali.sup=c(1, 4, 6, 10, 12, 13))

X2 = bicing[c("Start.weekday", "Period", "Metro.station.E", "Downtown.E",  "Account.type", "Weather.main", "Month", "Temperature")] 
```

```{r}
X2
```

The new dataframe has 4877 individuals ( I ) and 8 columns: the first 6 will be our active categorical variables for the MCA ( J ), while "Month" will be threated as a supplementary categorical variable, and "Temperature" as a supplementary quantitative variable.

Let's now set the correct types for the column variables

```{r}
summary(X2["Month"])
```

As we can see from above, the variable "Month" is threated as a quantitative variable, while it assumes only 8 discrete values as shown below:

```{r}
unique(X2[, "Month"])
```

To solve this issue we transform it into a categorical variable:

```{r}
X2[,"Month"] = as.factor(X2[,"Month"])
```

We now transform all the active variables into categorical ones:

```{r}
X2[,"Period"] = as.factor(X2[,"Period"])
X2[,"Start.weekday"] = as.factor(X2[,"Start.weekday"])
X2[,"Metro.station.E"] = as.factor(X2[,"Metro.station.E"])
X2[,"Downtown.E"] = as.factor(X2[,"Downtown.E"])
X2[,"Account.type"] = as.factor(X2[,"Account.type"])
X2[,"Weather.main"] = as.factor(X2[,"Weather.main"])
```

```{r}
print(unique(X2[, "Period"]))
print(unique(X2[, "Start.weekday"]))
print(unique(X2[, "Metro.station.E"]))
print(unique(X2[, "Downtown.E"]))
print(unique(X2[, "Account.type"]))
print(unique(X2[, "Weather.main"]))
```

Let's see now the summary of our dataframe

```{r}
summary(X2)
```

It's also possible to plot the frequency of variable categories

```{r}
for (i in 1:6) {
  plot(X2[,i], main=colnames(X2)[i],
       ylab = "Count", col="steelblue", las = 2)
  }
```

We are now ready to perform MCA

```{r}
?MCA
```

We use the MCA function of the FactoMineR package and we specify the number of dimensions to keep in the final results by means of the parameter ncp.

```{r}
bicing_mca = MCA(X2, ncp=10, quali.sup = 7,  quanti.sup=8, level.ventil=0.01, graph = FALSE)
```

## Questions

***What is the number of dimensions that this MCA generates? What does this value equal?***

```{r}
bicing_mca$eig
```

MCA generates 16 dimensions which all together explain 100% of the variance of the original data.

We know that the number of dimensions generated by MCA is given by $K - J$, where $K$ represents the total number of categories of the active categorical variables, while $J$ represents the number of active categorical variables.

In our case $J$ is equal to 6, while $K$ is equal to the total sum of the categories of each active categorical variable:
```{r}
unique_count = c()
for(col in 1:(length(colnames(X2)) - 2)){
  unique_count = c(unique_count, length(unique(X2[,colnames(X2)[col]])))
}
sum(unique_count)
```

Therefore, we should obtain $K-J=26-6=20$ dimensions/eigenvalues. 
However, as we are applying ventilation (of 1%), 4 categories as removed for being rare which leaves us with 16 categories.

For the rest of the exploitation, we will only keep the 10 first dimensions as we specified when computing MCA (through the `ncp` argument).

### Visualization and Interpretation

Let's now analyze the insight of the MCA computation.

#### Plots

**Interpret the first factorial plane (dimensions 1 and 2) by means of variables and categories**

Let's first interpret the variable categories plot on the first 2 dimensions:
```{r}
fviz_mca_var(bicing_mca, repel = TRUE, ggtheme = theme_minimal())
```
On this plot, we can see how much each variable categories are correlated to each others.
For example, we can see a strong negative correlation between `Satursday` and `Morning` as being opposed compared to the origin of the plot. This means that a very few profiles were combining those two variables, if not none.  
__Contextualization:__ A very few bike rentals were made on Satursday mornings, compared to other rentals.

Also, we can see that there is a relatively strong correlation between between `Friday` and `Wednesday` variables or between `Showers` and `Monday`.  
__Contextualization:__ Profiles of bike rentals of Friday and Wednesday are pretty similar, which could be due to populations going out at night and renting bikes to do so (as it is less frequent to go out at the beginning of the week).

About variables:
```{r}
fviz_mca_var(bicing_mca, choice="var", repel = TRUE, ggtheme = theme_minimal())
```
On this plot, we can see how each variables are correlating with the 2 first dimensions. 
For example, we can see that `Weather.main` is strongly correlated with the second dimension when `Temperature` is having a low correlation with both dimensions, thus not having a strong influence on this dimension. Also, there are variables such as `Period` or `Start.weekday` which have a mixed correlation with both dimensions.  

**What does it means ?** For a variable to have a strong correlation with a dimension means that among all variables, this one is having a strong influence on the dimension (to which the eigenvalue is a simple linear combination of each variables).  

#### Choosing dimensions
We can see that the computed dimension have all of them a relatively low portion of variation explanation (to not be biased during reading this plot, **please pay attention to the y axis range**):
```{r}
fviz_screeplot(bicing_mca, addlabels = TRUE, ylim = c (0, 10))
```

**Which percentage of variability is explained by the first two dimensions?**
For example, for the above interpretation of variables and categories, we used the two first dimensions. Even if they are the most explaining in terms of variance, they only account for 16% of variance in total:
```{r}
head(bicing_mca$eig, 2)
```

Therefore, these dimensions are not so representative of the whole variance.

